{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"../data/TUDataset\", name='COLLAB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Collab Dataset\n",
    "COLLAB is a scientific collaboration dataset. A graph corresponds to a researcher’s ego network, i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. A researcher’s ego network has three possible labels, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics, which are the fields that the researcher belongs to. The dataset has 5,000 graphs and each graph has label 0, 1, or 2.\n",
    "https://paperswithcode.com/dataset/collab \n",
    "https://networkrepository.com/COLLAB.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: COLLAB(5000)\n",
      "Num Graphs: 5000\n",
      "Num Nodes: 372474\n",
      "Num classes: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}')\n",
    "print(f'Num Graphs: {len(dataset)}')\n",
    "print(f'Num Nodes: {dataset.num_nodes}')\n",
    "print(f'Num classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42., 46., 39., 45.,  8., 45., 39., 45., 42., 45.,  7., 45., 45., 45.,\n",
       "        42., 47., 45., 45., 45., 45., 47., 42., 45., 45., 47., 45., 39., 45.,\n",
       "        42., 45., 45., 45., 45., 45., 45., 45., 45., 45., 45., 47., 47., 45.,\n",
       "        45., 45., 45., 45., 45., 42.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.utils.degree(dataset[4].edge_index[0], dataset[4].num_nodes)\n",
    "\n",
    "# dataset[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set CUDA\n",
    "device = \"cuda:0\" if (torch.cuda.is_available()) else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 0\n",
    "for data in dataset:\n",
    "    deg = torch_geometric.utils.degree(data.edge_index[1], num_nodes=data.num_nodes)\n",
    "    max_degree = max(max_degree, max(deg).item())\n",
    "# assign to one hot degree for each data (OneHotDegree receive maximum degree parameter)\n",
    "dataset.transform = torch_geometric.transforms.OneHotDegree(int(max_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_dataset = []\n",
    "for g in dataset:\n",
    "    cuda_dataset.append(g.to(torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_dataset[0].x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COLLAB(5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 0.8\n",
    "seed = 123\n",
    "\n",
    "num_split = round(len(dataset) * split)\n",
    "torch.manual_seed(seed)\n",
    "dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  4000\n",
      "Test:  1000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cuda_dataset[:num_split]\n",
    "test_dataset = cuda_dataset[num_split:]\n",
    "print('Train: ', len(train_dataset))\n",
    "print('Test: ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Graph neural network models\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "# pooling method (for readout layer)\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import SAGPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        # seed\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(data.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, data.num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = self.conv3(x, edge_index)        \n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/inyeoplee77/SAGPool/blob/master/networks.py\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, data, hidden_channels):\n",
    "#         super(GCN, self).__init__()\n",
    "#         # seed\n",
    "#         torch.manual_seed(42)\n",
    "#         self.conv1 = GCNConv(data.num_node_features, hidden_channels)\n",
    "#         self.pool1 = SAGPooling(hidden_channels, ratio=0.5)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.pool2 = SAGPooling(hidden_channels, ratio=0.5)\n",
    "#         self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.pool3 = SAGPooling(hidden_channels, ratio=0.5)\n",
    "        \n",
    "#         self.lin1 = Linear(hidden_channels*2, hidden_channels)\n",
    "#         self.lin2 = Linear(hidden_channels, hidden_channels//2)\n",
    "#         self.lin3 = Linear(hidden_channels//2, data.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "#         x1 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "        \n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "#         x2 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "        \n",
    "#         x = self.conv3(x, edge_index)\n",
    "#         x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "#         x3 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "#         # x = global_max_pool(x, batch)\n",
    "#         x = x1 + x2 + x3\n",
    "        \n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = F.relu(self.lin2(x))\n",
    "#         x = self.lin3(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data.x.to(torch.device(device)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return out, loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(492, 64)\n",
       "  (pool1): SAGPooling(GraphConv, 64, ratio=0.5, multiplier=1.0)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (pool2): SAGPooling(GraphConv, 64, ratio=0.5, multiplier=1.0)\n",
       "  (conv3): GCNConv(64, 64)\n",
       "  (pool3): SAGPooling(GraphConv, 64, ratio=0.5, multiplier=1.0)\n",
       "  (lin1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (lin3): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(dataset, 64)\n",
    "model.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sclab\\anaconda3\\Lib\\site-packages\\torch_geometric\\utils\\scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_acc: 0.7662 loss: 0.5372 test_acc: 0.3930\n",
      "epoch: 2 train_acc: 0.7740 loss: 0.6605 test_acc: 0.6050\n",
      "epoch: 3 train_acc: 0.8085 loss: 0.5898 test_acc: 0.4880\n",
      "epoch: 4 train_acc: 0.8053 loss: 0.6668 test_acc: 0.6180\n",
      "epoch: 5 train_acc: 0.8033 loss: 0.4897 test_acc: 0.4470\n",
      "epoch: 6 train_acc: 0.8023 loss: 0.3882 test_acc: 0.4740\n",
      "epoch: 7 train_acc: 0.8115 loss: 0.3308 test_acc: 0.4630\n",
      "epoch: 8 train_acc: 0.8080 loss: 0.3953 test_acc: 0.4960\n",
      "epoch: 9 train_acc: 0.8230 loss: 0.3040 test_acc: 0.4570\n",
      "epoch: 10 train_acc: 0.8317 loss: 0.2975 test_acc: 0.5230\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_train_acc = []\n",
    "list_test_acc = []\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    out, loss = train(model, train_loader)\n",
    "    train_acc = test(model, train_loader)\n",
    "    test_acc = test(model, test_loader)\n",
    "    \n",
    "    list_train_acc.append(round(train_acc, 4))\n",
    "    list_test_acc.append(round(test_acc, 4))\n",
    "    list_loss.append(round(loss.item(), 4))\n",
    "\n",
    "    print(f\"epoch: {epoch+1} train_acc: {train_acc:.4f} loss: {loss:.4f} test_acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free cuda memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.cpu()\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 190668], y=[64], num_nodes=4960, x=[4960, 492], batch=[4960], ptr=[65])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for train in train_loader:\n",
    "    print(train)\n",
    "    print(train.x)\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
